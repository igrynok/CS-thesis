 \documentclass[11pt, oneside,a4paper]{book}
\pagestyle{headings}

\input{preamble}

% A B S T R A C T
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\chapter*{\centering Abstract}
\begin{quotation}
\noindent 
Nowadays the number of documents in the World Wide Web grows extremely fast. Tools that can facilitate information retrieval present a particular interest in the modern world. We believe that considering meta information helps to build enhanced search systems that can facilitate information retrieval. Particularly, we target information retrieval for scientific articles. We consider citations in scientific articles as important text blocks summarizing or judging previous scientific findings, assisting in creating a new scientific work. 

We propose Citation Search Engine a software system that makes an attempt to automatically extract, index and aggregate citations from a set of scientific articles in PDF format. Besides it analyses the results of the deployment of the system on a collection of scientific papers. It evaluates searching capabilities of the system by comparing with alternative approaches.

\end{quotation}
\clearpage


% C O N T E N T S 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% NEW CHAPTER %%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{cha:introduction}

\section{Thesis statement}
Searching over large collection of documents by hand is time inefficient. The common way of making search effective in terms of time is indexing documents in advance. Before indexing procedure, documents should be parsed and analyzed. In this work we aim to build search system over collection of scientific articles in PDF format. Scientific articles are different from ordinary documents in following sense. First, scientific articles in PDF need to be converted into textual representation. Secondly, scientific articles have a certain layout format, where each section need to be considered separately. In Citation Search Engine, we do not aim to index full text of articles, instead we aim to take advantage of indexing extracted meta-information. We believe that considering meta-information might enhance a search system. We particularly focused on extracting citations from scientific articles. In general, a citation composed of two parts: a cited text in the body of a document and a bibliographic link in a reference section of a document. In Citation Search Engine we index both parts separately allowing user to make a choice between different indexed collections. We also store context of a citation to better understanding of the idea of the cited text. Citation context is a text framing cited text. Bibliographic links identifies the cited origin paper. By finding bibliographic links pointing to the same original source we can merge citations related to the same paper. Such functionality is especially useful to automatically find citations of a given article in other articles.   

\section{Highlights}
Following are highlights of this work:
\begin{itemize}
	\item Design and implement the Citation Search Engine.
	\item Deploy the system on the given collection of scientific papers
	\item Make evaluation by comparing with alternative approaches
	\item Analyze results, define future work
\end{itemize}

\section{Outline}
The rest of the paper structured as follows:
\begin{description}
	\item[Chapter \ref{cha:introduction}] The chapter Gives a high overview of the architecture of a typical web search engine. It describes the main steps to construct an inverted index.
	\item[Chapter \ref{cha:related-work}] The chapter surveys the research related to citations in scientific publications. It makes an overview of two popular academic search engines: Google Scholar and CiteSeer.
	\item[Chapter \ref{cha:CS}] The chapter describes the design of Citation Search Engine. It first shows overall architecture of the proposed system and then shows details of implementation of each component. 
	\item[Chapter \ref{cha:validation}] The chapter describes the deployment process and analysis the result of setting up the system on the given collection of scientific articles.
	\item[Chapter 5] Evaluation
	\item[Chapter \ref{cha:conclusion}] Contains conclusion and possible future work. 
\end{description}
\pagebreak

\section{Typical web search engine} 

Figure \ref{fig:web-search} illustrates a high level architecture of a standard web engine. It consists of three main components:
\begin{itemize}
	\item Crawler
	\item Indexer
	\item Index Storage	
	\item Search interface
	
\end{itemize} 

  \begin{figure}[htp]
    \centering
    \includegraphics[scale=0.55, trim = 0mm 140mm 0mm 10mm,]{typical-se}
    \caption{A high-level architecture of a typical web search engine}
    \label{fig:web-search}
  \end{figure}

Web Crawler is a program that browses the World Wide Web reading the content of web pages in order to provide up-to-date data to Indexer. Indexer decides how a page content should be stored in an index storage. Indices help to quickly query documents from the index storage. Users can search and view query results through the Search Interface. When a user makes a query the search engine analyzes its index and returns best matched web pages according to specific criteria. 

Web crawlers that fetch web pages with the content in the same domain are called focused or topical crawlers. An example of a focused crawler is an academic-focused crawler that crawls scientific articles. Such crawlers become components of focused search engines. Examples of popular academic search engines are Google Scholar and CiteSeer. Chapter \ref{cha:related-work} gives an overview of these search engines.  

\section{Inverted index}
Search engines like CiteSeer or Google Scholar deal with a large collection of documents. The way to avoid linear scanning the text of all documents for each query is to \emph{index} them in advance. Thereby we are coming to the concept of \emph{inverted index}, which is a major concept in information retrieval. The term \emph{inverted index} comes from the data structure storing a mapping from content, such as words or numbers, to the parts of a document where it occurs. Figure \ref{fig:inverted-index} shows the basic idea of an inverted index. We have a dictionary of terms appearing in the documents. Each term maps to a list that records which documents the term occurs in. Each item in the list, conventionally named as \emph{posting}, records that a term appears in a document, often it records the position of the term in the document as well. The dictionary on Figure \ref{fig:inverted-index} has been sorted alphabetically and each postings list is sorted by document ID. Document ID is a unique number that can be assigned to a document when it's first encountered. \\
  \begin{figure}[htp]
    \centering
    \includegraphics[scale=0.35, trim = 0mm 30mm 0mm 10mm]{inverted-index}
    \caption{Example of an inverted index. Each term in a dictionary maps to a posting list consisting of document IDs, where this term occurs. Dictionary terms are sorted alphabetically and posting lists are sorted by document IDs}
    \label{fig:inverted-index}
  \end{figure}
  
 The construction of the inverted index has following steps:
 
\begin{enumerate}
	\item Documents collection
	\item Breaking each document into tokens, turning a document into a list of tokens
  	\item Linguistic preprocessing of a list of tokens into normalised list of tokens
	\item Index documents by creating  an inverted index, consisting of a dictionary with terms and postings
\end{enumerate}

First step of the index construction is documents collection. It aims at obtaining a set of documents containing a sequence of characters. The input to the indexing process could be digital documents that are bytes in a file or a web server. Consider, for example, a collection of PDF files. First, we need to correctly decode out characters of the binary representation. Next we should determine a document unit. For example, it might be a chapter in a book, or a paragraph in a scientific article. 

The next step after getting the sequences of characters in the document units, is to break up documents into \emph{tokens}. Tokens can be thought of as the semantical units for processing. For example, it might be a word or a number. During tokenisation some characters like punctuations can be thrown out. 

Here is a tokenisation example:

\hspace{100pt}\textcolor[rgb]{0,0,1}{ Input: Sometimes, I forget things.}

\hspace{100pt}\textcolor[rgb]{0,0,1}{Output: \framebox{Sometimes} \framebox{I} \framebox{forget} \framebox{things}} \\

The third step is normalization. Consider an example of querying the word \emph{co-operation}. A user might also be interested in getting documents containing \emph{cooperaion}. \emph{Token normalisation} is a process of turning a token into a canonical form so matches can occur despite lexical differences in the character sequences. One way of token normalisation is keeping relations between unnormalized tokens, which can be extended to manual constructed synonym lists, such as \emph{car} and \emph{automobile}. The most standard way of token normalization however is creating \emph{equivalence classes}. If tokens become identical after applying a set of rules then they are in the equivalence classes. Consider the following common normalization rules that are often used:

\begin{description}
	\item[Stemming and Lemmatization] Words can be used in different grammatical forms. For instance, \emph{organize}, \emph{organizes}, \emph{organizing}.However in many cases it sounds reasonable for one of these words to return documents contain other forms of the word. The goal of stemming and lemmatization is to reduce the form of the word to a common base form. 
	
Here is an example: 
	
\hspace{100pt} \textcolor[rgb]{0,0,1}{ am, are, is = \textgreater be }

\hspace{100pt} \textcolor[rgb]{0,0,1}{car, cars, car's, cars'  =\textgreater car}

The result of applying the rule to the sentence:

\hspace{100pt} \textcolor[rgb]{0,0,1}{three frogs are flying   =\textgreater three frog be fly}

Stemming and lemmatization are closely related concepts however there is a difference. \emph{Lemmatization} usually refers to finding a \emph{lemma}, common base of a word, with the help of a vocabulary, morphological analysis of a word and requires understanding the context of a word and language grammar. \emph{Stemming} however operates with a word without knowing its context and thereby can't distinguish that the same words have different meanings depending on the context. 

Here is an example:

\hspace{10pt} \textcolor[rgb]{0,0,1}{ better = \textgreater good }, can only be matched by lemmatization since \ins{it} requires dictionary look-up

\hspace{10pt} \textcolor[rgb]{0,0,1}{writing  =\textgreater write}, can be matched by both lemmatization and stemming

\hspace{10pt} \textcolor[rgb]{0,0,1}{meeting  =\textgreater meeting(noun) or to meet(verb)}, can be matched only by lemmatization since it requires the word context 

In general\ins{,} stemmers are easier to implement and run faster. The most common algorithm for stemming is \emph{Porter's} algorithm \cite{Porter:1997:ASS:275537.275705}.
	  
	\item[Capitalization/Case-Folding] A simple strategy is to reduce all letters to a lower case, so that sentences with \emph{Automobile} will match to queries with \emph{automobile}. However this approach would not be appropriate in some contexts like identifying company names, such as \emph{General Motors}. Case-folding can be be done more accurately by a machine learning model using more features to identify whether a word should be lowercased.
	\item[Accents and Diacritics]  Diacritics in English language play an insignificant role and simply can be removed. For instance \emph{clich\'e} can be substituted by \emph{cliche}. In other languages diacritics can be part of the writing system and distinguish different sounds. However, in many cases, users can enter queries for words without diacritics.
\end{description}

The last step of building the inverted index is sorting. The input to indexing is a list of pairs of normalized tokens and documents IDs for each document. Consider an example of three documents with their contents:
\begin{itemize}
	\item Document 1: Follow the rules.
	\item Document 2: This is our town.
	\item Document 3: The gates are open.
\end{itemize}

 After applying tokenasation and normalisation steps of the listed documents the input to the indexing is shown in Table \ref{tab:input-indexing}. 
\begin{table}[ht]
\begin{center}
\begin{tabular}{ | l | l | } 
 \hline
 \textbf{Term} & \textbf{DocumentID} \\  \hline
 follow & 1\\ 
 the & 1\\
 rule & 1\\
 this & 2\\ 
 be & 2\\
 our & 2\\
 town & 2\\
 the & 3\\ 
 gate & 3\\
 be & 3\\
 open & 3\\        
 \hline
\end{tabular}
\caption{Input to the indexing algorithm is a list of pairs of a term and document ID, where this term occurs.}
\label{tab:input-indexing}
\end{center}
\end{table}
The indexing algorithm sorts the input list so that the terms are in alphabetical order as in Table \ref{tab:indexing-sorting}. 
 \begin{table}[ht]
\begin{center}
\begin{tabular}{ | l | l | } 
 \hline
 \textbf{Term} & \textbf{DocumentID} \\  \hline
 be & 2\\ 
 be & 3\\
 follow & 1\\
 gate & 3\\
 open & 3\\
 our & 2\\
 rule & 1\\
 the & 1\\
 the & 3\\
 this & 2\\
 town & 2\\   
 \hline
\end{tabular}
\caption{Indexing algorithm sorts all terms in a alphabetical order. The result is a list of sorted terms with document IDs}
\label{tab:indexing-sorting}
\end{center}
\end{table}
Then it merges the same terms from the same document by folding two identical adjacent items in the list. And finally instances of the same term are grouped and the result is split into a dictionary with postings, as shown in Table \ref{tab:indexing-merging}.
 \begin{table}[ht]
\begin{center}
\begin{tabular}{ | l | l | } 
 \hline
 \textbf{Term} & \textbf{Postings} \\  \hline
 be & 2 3\\ 
 follow & 1\\
 gate & 3\\
 open & 3\\
 our & 2\\
 rule & 1\\
 the & 1 3\\
 this & 2\\
 town & 2\\   
 \hline
\end{tabular}
\caption{Indexing algorithm groups the same terms with creating postings. The result is a dictionary with terms as keywords and values as postings.}
\label{tab:indexing-merging}
\end{center}
\end{table}


%%Since a term generally occurs in a number of documents, this data organization already reduces the storage requirements of the index. 

\section{Dynamic indexing}
So far we assumed that document collection is static. However there are many cases when the collection can be updated, for example, by adding new documents, deleting or updating existing documents. Simple way to deal with dynamic collection is to reconstruct the inverted index from scratch. This might be acceptable if the changes made in the collection are small over time and the delay in making new documents searchable is not critical. However if there is one of the aforementioned conditions is violated, one might be interested in another more dynamic solution like keeping an auxiliary index. Thus we have a large main index and we keep auxiliary index for changes. The auxiliary index is kept in memory. Every time a user makes a query the search runs over both indexes and results are merged. When the auxiliary index becomes too large it can be merged with the main index.

\section{Retrieving search results}
When a user makes a query it would be good to give her back a result document containing all terms in the query, so that terms are located close to each other in the document. Consider an example of querying a phrase containing 4 terms. The part of the document that contains all terms is named a \emph{window}. The size of the window is measured in number of words. For instance the smallest window for 4-term query will be 4. Intuitively, smaller windows represent better results for users. Such window can become one of the parameters ranking a document in the search result. If there is no document containing all 4 terms, a 3-term phrase can be queried. Usually search systems hide the complexity querying from the user by introducing \emph{free text query parsers} so a user can make only one query.

\chapter {Related Work}
\label{cha:related-work}

\section{Citations in scientific publications}

Citations are the subject of many interesting scientific studies. 

Bradshaw et al. \cite{bradshaw} showed that citations provide many different perspectives on the same article. They believe that citation provide means to measure the relative impact of articles in a collection of scientific literature. In their work the authors improved the relevance of documents in the search engine results with a method called Reference Directed Indexing. Reference Directed Indexing (RDI) is based on a comparison of the terms authors use in reference to documents.

Bertin and Atanassova \cite{marc1} \cite{M_automaticannotation} \cite{autocit} automatically extract citations and annotate them using a set of semantic categories. In \cite{M_automaticannotation} and \cite{marc1} they used linguistic approach, which used the contextual exploration method, to annotate
automatically the text. In \cite{autocit} they proposed a hybrid method for the extraction and characterization of citations in scientific papers using machine learning combined with rule-based approaches. 

There are several studies that used citations to evaluate science by introducing map of science. Map of science graphically reflects the structure, evolution and main contributors of a given scientific field \cite{map1} \cite{Klavans:2009:TCM:1527090.1527095} \cite{DBLP:journals/corr/abs-1202-1914} \cite{Small:1999:VSC:308915.308932}. 

Kessler \cite{ASI:ASI5090140103} first used the concept of bibliographic coupling for document clustering. To build a cluster of similar documents Kessler used a similarity function based on the degree of bibliographic coupling. Bibliographic coupling is a number of citations two documents have in common. The idea was developed further by Small in co-citation analysis \cite{ASI:ASI4630240406}. Later co-citation analysis and bibliographic coupling was used by Larson \cite{larson1996bibliometrics} for measuring similarity of wwww pages.

Another approach is to use citations to build summaries of scientific publications. There are three categories of summaries proposed based on citations: overview of a research area (multi-document summarization) \cite{Nanba:1999:TMS:1624312.1624351}, impact summary (single document summary with citations from the scientific article itself) \cite{mei-zhai:2008:ACLMain} and citation summary (multi- and single document summarization, in which citations from other papers are considered) \cite{Qazvinian:2008:SPS:1599081.1599168}. In work by Nakov et al. citations have been used to support automatic paraphrasing \cite{Nakov04citances:citation}. 

An expert literature survey on citation analysis was made by Smith \cite{smith}, she reviewed hundred of scientific articles on this topic. 

\section{Popular academic search engines}
\paragraph{CiteSeer\textsuperscript{x}}

CiteSeer\textsuperscript{x} is built on the concept of citation index. The concept of citation index was first introduced by Eugene Garfield in \cite{garfield1964science}. In terms of Eugene Garfield citations are bibliographic links or referrers linking scientific documents. In his work Eugene Garfield proposed an approach where citations between documents were manually cataloged and maintained so that a researcher can search through listings of citations traversing citation links either back through supporting literature or forward through the work of later researchers \cite{bradshaw2002reference}.

Lawrence et al. automated this process in CiteSeer\textsuperscript{x} \footnote{CiteSeer, http://citeseerx.ist.psu.edu/} \cite{Giles:1998:CAC:276675.276685}, a Web-based information system that permits users to browse the citation links between documents as hyperlinks. CiteSeer\textsuperscript{x} automatically parses and indexes publicly available scientific articles found on the World Wide Web. 

CiteSeer\textsuperscript{x} is built on top of the the open source infrastructure SeerSuite \footnote{SeerSuite, http://citeseerx.sourceforge.net/} and uses Apache Solr \footnote{Apache Solr. http://lucene.apache.org/solr/} search platform for indexing documents. It can extract meta information from papers such as title, authors, abstract, citations. The extraction methods are based on machine learning approaches such as ParseCit \cite{Councill08parscit:an}. 
CiteSeer\textsuperscript{x} currently has over 4 million documents with nearly 4 million unique authors and 80 million citations. 

CiteSeer\textsuperscript{x} indexes citations more precisely bibliographic links or referrers while in Citation Search Engine we intend to index not only bibliographic links but also cited text in a body of a document. If by indexing bibliographic links CiteSeer\textsuperscript{x} mainly aims to simplify navigation between linked documents, in Citation Search we focus on simplifying retrieval of documents containing a text of interest.  
\\ \\
\textbf{Google Scholar} 
Google Scholar is a freely accessible web search engine that makes full-text or metadata indexing of scientific literature \footnote{Google Scholar, http://scholar.google.ch/}. Besides the simple search Google Scholar proposes following features. Unique ranking algorithm, an algorithm that ranks documents ``the way researchers do, weighing the full text of each document, where it was published, who it was written by, as well as how often and how recently it has been cited in other scholarly literature'' \footnote{https://scholar.google.com/scholar/about.html}."Cited by" feature allows to view abstracts of articles citing the given article. "Related articles" feature shows the list of closely related articles. It is also possible to filter articles by author name or published date.
Google Scholar contains roughly 160 million of documents by May 2014 \cite{orduna2014size}. 

%In which we understand what the problem is in detail.

\chapter {Citation Search Engine}
\label{cha:CS}

\section{System overview}

The overall architecture of Citation Search Engine is shown on Figure \ref{fig:cs}. There are three main operations performed by Citation Search Engine: parsing PDF files, indexing documents and querying \chg{on}{the} resulted indexes. Correspondingly\ins{,} there are three major components responsible for accomplishment of these operations: \emph{Parser}, \emph{Indexer Solution}\ho{Indexer Solution is not a good name for a component. Just name it the Indexer} and \emph{Search Web App}. The system has two more components for storing data: \emph{Indexes Storage} for storing indexes and \emph{Meta Information Storage} for storing meta information \ho{First, the names are pretty self-explanatory. Second: Why there are two different storage? You have to justify.}. For the convenience, the workflow of the system is numbered and will be explained below.

  \begin{figure}[htp]
    \centering
    \includegraphics[scale=0.5, trim = 0mm 20mm 0mm 20mm]{cs}
    \caption{\chg{Search Engine}{change this to the name of the tool ;)} high level architecture \ho{The architecture diagram is not clear because it is a mix between an activity diagrams and a component diagram. There should be one diagram for each.}}
    \label{fig:cs}
  \end{figure}

\ho{There should be two diagrams. One component diagram and one activity diagram. Then you should explain each one one its own.}

\chg{F}{The f}irst operation performed by the system is indexing. \del{In the indexing process collection of PDF files is provided to \emph{Parser}} (1). \ins{The} \emph{Parser} \chg{parses}{converts} \ins{the} PDF files into text and extract meta \ho{what meta information} information from text. It then packs data about citations into documents and publishes obtained documents to \emph{Indexer Solution} (2) which indexes and stores indexes in \emph{Indexes Storage} (2'). Meta information extracted from textual representation of PDF files that doesn't require indexing is stored in \emph{Meta Information Storage} (3). \ho{the whole paragraph is not clear. Try to simplify it by just enumerating the steps in an another activity diagram where you explicitly specify the input and output if each step.}

In the querying process user searches for some phrase using \emph{Search Web App} user interface. The phrase is analyzed by \ins{the} \emph{Indexer Solution}. \emph{Indexer Solution} finds documents \ho{what documents?? Remember that your thesis should be self-explanatory. I know that the term ``documents" is used when dealing with Solr, but the reader doesn't. Try to be more clear.} matching the query phrase in \emph{Indexes Storage} (2') and sends them to \emph{Search Web App} (4). If \ins{the} user is interested in getting details regarding \chg{the}{a} specific reference\ins{,} the information \ho{what information?} will be looked up in \ins{the} \emph{Meta Information Storage} (5). Finally, the result is shown to the user as \del{a} an \chg{html}{HTML} page (6). 

\ins{The} \chg{N}{n}ext sections of \chg{the}{this} chapter describe \ins{the} implementation of each component in detail and show the reasons \chg{of}{behind} choosing a particular solution.       

\section{Parser}

It is practical to divide the work of \ins{the} \emph{Parser} into two phases: \emph{Processing} and \emph{Publishing}, as \chg{on}{in} Figure \ref{fig:parser}. The output of \ins{the} \emph{Processing} phase is \ins{the} input to \ins{the} \emph{Publishing} phase. \del{ \emph{Processing} phase aims to parse PDF-files and prepare documents with extracted information for publishing. Next parts of this section describes \emph{Processing} and \emph{Publishing} phases in detail. 
}
  \begin{figure}[htp]
    \centering
    \includegraphics[scale=0.5, trim = 0mm 140mm 0mm 10mm]{parser}
    \caption{Parser workflow}
    \label{fig:parser}
  \end{figure}

\subsection{Processing}
\ho{give this phase a better title and use it accordingly in the text.}
The main role of \emph{Processing} phase is to parse scientific articles into text and extract citations and bibliographic references to create documents for publishing. 
\del{In common case }\chg{p}{P}arsing PDF-files from different sources is a very challenging task as there is a large number of scientific articles in different formats. Besides that\ins{,} there exist scientific articles written decades ago which can use different encoding algorithms \chg{than}{from} articles created recently. Thereby\ins{,} building a universal parser is \chg{practically impossible}{very hard in practice}. In our case\ins{,} we try to identify common patterns covering \ins{the} majority of \ins{the} scientific article formats or at least \ins{the} formats found in our collection of articles.  

\emph{Processing} phase starts with recursively walking though \ins{the} directory tree of the collection library. While walking \chg{though}{through} \ins{the} directory\ins{, the} \emph{Parser} filters non-pdf files and parses and processes each PDF-file separately. We use\del{d} Apache PDFBox library \footnote{Apache PDFBox, https://pdfbox.apache.org/} to parse PDF-files into text. The library extracts full text from PDF-file\ins{s,} but without any hints on the initial structure of the article. In our case\ins{,} to find citations and bibliographic references in text\ins{,} we need to look them up in different parts of the article\chg{, so it was}{. We} designed an algorithm to break \ins{the} PDF-text into sections.

Generally\ins{,} we are interested in identifying a body of the document where we can find citations and reference\del{s} block\ins{s} where we can find bibliographic links. One way of finding these sections can be using keywords that might signify the beginning or the end of some section\ins{s}. Based on \ins{those} keywords\ins{,} on\ins{e} can extract different sections of a document. Figure \ref{fig:template} shows a sample text of a parsed pdf-document with keywords. 
  
One can notice \ins{the} following characteristics of scientific articles:
\begin{itemize}
	\item \ins{The} Body of a document comes before \ins{the} references section\ins{.}
	\item \ins{The} Appendix or Author's biography sections can come after \ins{the} references section\ins{.}
	\item Each document contains \chg{an}{the} \ins{``}Abstract\ins{"} and \ins{the ``}References\ins{"} words and might contain \ins{the ``}Appendix\ins{"} word. We call these words keywords. 
\end{itemize}  

\chg{In common case }{The }keywords can be written in different formats, \chg{for example,}{like} using upper or lower case\ins{s}. Table \ref{tab:keywords} illustrates \ins{some} variations of \ins{the} keywords\del{ in the beginning of document sections}.

\begin{table}[ht]
\begin{center}
\begin{tabular}{ | l | l | l | } 
 \hline
 body & references & appendix \\  \hline
 Abstract & References & Appendix \\ 
 ABSTRACT & References: & APPENDIX \\
 & REFERENCES & \\  
 \hline
\end{tabular}
\caption{Keywords identifying different sections in a document}
\label{tab:keywords}
\end{center}
\end{table}

After breaking \chg{down a document}{a document down} into sections as show\ins{n} \chg{on}{in} Figure \ref{fig:template}, \ins{the} text \chg{in sections are}{is} presented in one-column format. There are two aspects regarding this format. First, sentences can be \chg{splitted}{split} by new line symbol\ins{s} \chg{in}{at} the end of a line. Second\del{ly}, words can be \chg{splitted}{split} by dash symbol \chg{in}{at} the end of a line. \chg{It was}{We} introduce a normalization step where new lines are substituted by white spaces and dashes are removed in the end of a line to obtain \chg{continues}{continuous} text.

  \begin{figure}[htp]
    \centering
    \includegraphics[scale=0.6]{template}
    \caption{Sample text of the parsed scientific article. Keywords help to break the document into sections.\ho{Why isn't this figure centred?}}
    \label{fig:template}
  \end{figure}

As a result of \ins{the} normalization step\ins{,} we have a document divided into body and references sections. Before searching citations in the body of a document\chg{ it is reasonable to }{, we }break \ins{the} body into sentences. In general, breaking text into sentences is not an easy task. Consider a simple example with a period. Period not only indicates the end of a sentence but also can \ins{be} encounter\ins{ed} in\ins{side} \chg{a }{the} sentence itself, like an item of a numbered list or a name of a scientist. Besides, not all sentences end \chg{by}{with a} period, \chg{for example,}{like the} title of a section or an item of a list. \chg{It was}{We} used Stanford CoreNLP library that employs natural language and machine learning techniques to extract sentences \cite{stanford.nlp}. 

Next, we search for \ins{the} citations in \chg{a}{the} body\del{ of a document} and \ins{for the }bibliography links in \chg{a}{the} references section. When an author makes a citation \chg{it}{he} puts a link to a bibliographic reference in the sentence. It is common to use square brackets ([ and ]) to make a link to a bibliographic reference in the sentence. Thus, detecting square brackets in the text we can identify citations \ho{convoluted. Try to write sentences in the normal order}. After analyzing some set of articles\chg{ it was revealed}{, we found multiple} \del{frequent} patterns in using square brackets for \chg{citing}{citations}\ins{, as} \chg{showed}{shown} in Table \ref{tab:citations}. 

\begin{table}[ht]
\begin{center}
\begin{tabular}{ | l | p{7cm} | } 
 \hline
 Patterns of using [ ] & Example in text \\  \hline
 {[}21{]} & Our conclusion is that, contrary to prior pessimism \textbf{[21]}, [22], data mining static code attributes to learn defect predictors is useful. \\ 
 {[}20, 3, 11, 17{]} & In the nineties, researchers focused on specialized multivariate models, i.e., models based on sets of metrics selected for specific application areas and particular development environments \textbf{[20, 3, 11, 17]}. \\
 {[}24, Sections 6.3 and 6.4 {]}& Details on the life-cycle of a bug can be found in the BUGZILLA documentation \textbf{[24, Sections 6.3 and 6.4]}. \\
 {[}PJe02{]} & In a lazy language like Haskell \textbf{[PJe02]} this is not an issue - which is one key reason Haskell is very good at defining domain specific languages.  \\
 \hline
\end{tabular}
\caption{Frequent patterns in using square brackets ([ and ])for citing}
\label{tab:citations}
\end{center}
\end{table}


Further we need to extract bibliographic links from \chg{a}{the} references section. For that we studied most common variants of composing \ins{the} references sections. Table \ref{tab:references} \chg{surmises}{summarises} these findings. To extract bibliographic links we make \ins{a} list of regular expressions matching one of those patterns listing in Table \ref{tab:references}. Extracting bibliographic links allow\ins{s} us to match citations with bibliographic links.

\begin{table}[h]
\begin{center}
\begin{tabular}{ | p{12cm} | } 
 \hline
 References section templates\\  \hline
 \textbf{[1]} J. Bach. Useful features of a test automation system (partiii) \ldots \newline
 \textbf{[2]} B. Beizer. Black-Box Testing. John Wiley and Sons, \ldots \newline
 \ldots \\ \hline 
 \textbf{1.}  J. R. Hobbs,  Granularity, Ninth International Joint Conference \ldots \newline
  \textbf{2.}  W. Woods, What's in a Link:  Foundations for Semantic Networks, \ldots \newline
  \ldots \\ \hline
  \textbf{[1].}   Arnold, R.S., Software Reengineering, ed. \ldots \newline
  \textbf{[2].}   Larman, C., Applying  UML and Patterns. 1998, \ldots \newline  
  \ldots \\ \hline
  \textbf{[ASU86]} A. V. Aho, R. Sethi, and J. D. Ullman. Compilers: \ldots \newline
  \textbf{[AU73]} A.V. Aho and J.D. Ullman. The theory of parsing, translation \ldots \newline 
  \ldots \\
 \hline
\end{tabular}
\caption{Frequent patterns of writing references block}
\label{tab:references}
\end{center}
\end{table}

The pipeline of processing stage\ins{s} described above \ins{is} depicted on Figure \ref{fig:proc}. As seen from Figure \ref{fig:proc} to complete the processing stage we need to perform one more step: extracting titles from bibliographic links. The objective point of extracting titles from bibliographic links is to collect citations referred to the the same source (scientific article). In general case, different formats of bibliographic links can identify the same source or scientific article. For example, an article may have different editions, published in different journals in different years or simply different authors may use different style formatting. What we consider to be identical for all bibliographic links citing the same paper is the paper's title.   

  \begin{figure}[htp]
    \centering
    \includegraphics[scale=0.5, trim = 0mm 165mm 0mm 10mm]{processing}
    \caption{Pipeline of processing stage}
    \label{fig:proc}
  \end{figure} 

\paragraph{Processing bibliographic links}
As usual we try to recognize common patterns \chg{belonging to}{covering the} majority of bibliographic links. Table \ref{tab:bibl.links} shows some examples of bibliographic links. First, \chg{it was noticed}{we noticed} that if \ins{a} bibliographic link contains some sort of quotes, for example, double quotes \ins{(}``''\ins{)} or single quotes \ins{(}`'\ins{)}, then \ins{it is} highly probabl\chg{y}{e} that a title is enclosed by these quotes. Then, \chg{some observations were made}{we made some observations} for bibliographic links without quotes. Very often, a bibliographic link \ins{is} structured as follows: it begins by listing \ins{the} paper's authors, then \chg{paper's}{the} title\del{ goes} \ins{,} and then comes the rest of the link (see Figure \ref{fig:link}). It turned out that Core NLP library used for breaking text into sentences is good in dividing a link into parts according to our view. In most of cases it's enough to take the second part of the bibliographic link to be a title.   

\begin{table}[h]
\begin{center}
\begin{tabular}{ | p{12cm} | } 
 \hline
 Conradi, R., Dyba, T., Sjoberg, D.I.K., and Ulsund, T., "Lessons learned and recommendations from two large norwegian SPI programmes." Lecture notes in computer science, 2003, pp. 32-45."
 \\ \hline 
 P. Molin, L. Ohlsson, `Points \& Deviations - A pattern language for fire alarm systems,' to be published in Pattern Languages of Program Design 3, Addison-Wesley.
 \\ \hline
  R. P. Wilson and M. S. Lam. Effective context sensitive pointer analysis for C programs. In PLDI, pages 1–12, June 1995. 289 
  \\ \hline
  Allen, Thomas B. Vanishing Wildlife of North America. Washington, D.C.: National Geographic Society, 1974.
  \\ \hline
  I. Herraiz, J. M. Gonzalez-Barahona, and G. Robles. Towards a Theoretical Model for Software Growth. In Proceedings of the 4th International Workshop on Mining Software Repositories, Minnesotta, USA, May 2007.
  \\
 \hline
\end{tabular}
\caption{Some examples of bibliographic links}
\label{tab:bibl.links}
\end{center}
\end{table}

  \begin{figure}[htp]
    \centering
    \includegraphics[scale=0.55, trim = 0mm 165mm 0mm 10mm]{link}
    \caption{Structure of a bibliographic link}
    \label{fig:link}
  \end{figure}     

\subsection{Publishing}

There are two systems where documents will be published: Solr and MongoDB. As already explained above Solr is used for indexing citations and MongoDB for storing meta-data. Some reasonings in favor of using MongoDB additionally to Solr storage will be given below.

\paragraph{Publishing to Solr}
The common way to interact with Solr is using REST API. Solr provides client libraries for many programming languages to handle interactions with Solr's REST API. In our project we used SolrJ client library for Java language. This library abstract interaction with Solr into java objects instead of using typical XML request/response format. Basic Solr storage unit is called document and SolrJ has document abstraction implementation. For every detected citation we compose a document to publish. Figure \ref{fig:doc} represents a structure of documents we publish to Solr.

  \begin{figure}[htp]
    \centering
    \includegraphics[scale=0.2, trim = 0mm 20mm 0mm 10mm]{doc}
    \caption{Document structure publishing to Solr}
    \label{fig:doc}
  \end{figure} 

Every document representing one citation consist of following fields:
\begin{itemize}[noitemsep]
  \item id: document unique id, mandatory field for publishing to Solr
  \item text: text of the citation that we want to index
  \item context: citation with a text framing it, we take 1 sentence before and 1 after the citation 
  \item path: URL of a document where citation was found
  \item references: list of bibliographic links from references section matching this citation
\end{itemize}

Lets have a look at how Solr stores documents. Solr uses NoSQL like data storage, but actually it is even more limited than traditional NoSQL databases. Indeed, the data stored in Solr storage is very `flat', which means that Solr doesn't allow to store even hierarchical data \cite{use.solr1}, \cite{use.solr.2}. In our case along with the references we intent to store a title of the scientific article parsed from the reference string, so later we can aggregate citations referred to the same scientific article. /*It's worth to say that citation itself can refer to multiple scientific articles*/ We are also interested in a solution that doesn't require reviewing all Solr documents to find citations referred to the same scientific article as it will be too slow and will decrease quality of user experience. Thus we are coming to the external storage solution that will keep titles of scientific articles and all citations referred to a specific article. As there are few relations in our data and we would like to have a scalable solution it was decided to use MongoDB as an external storage.     

\paragraph{Publishing to MongoDB} 
MongoDB is a document-oriented NoSQL database that stores data in JSON-like documents with dynamic schema \footnote{MongoDB database, http://www.mongodb.org/}. To connect to database we used Java driver provided by MongoDB. Although MongoDB is a `schemaless' database we adhere to JSON structure of a document showed on Listing \ref{lst:json}. 
The json document consist of following fields:
\begin{itemize}[noitemsep]
	\item id: document id, field automatically assigned by MongoDB
	\item title: title of a scientific article
	\item cittations: citations with its references of the scientific article identifying by title field 
\end{itemize} 

Every time while sending a new citation with paper title to MongoDB we check if document with the same title already exist. If so we add a new citaion to this document otherwise create a new document. \\

\begin{minipage}{\linewidth}
\begin{lstlisting}[label={lst:json}, caption={Sample document stored in MongoDB}]
{ 
	"_id" : ObjectId("547ef1b219795f049d6a0ad0"), 
	"title" : "Re-examining the Fault Density-Component Size  Connection", 
	"citations" : [ 
	{ 
		"citation" : "Hatton, [19], claims that there is “compelling empirical 
					  evidence from disparate sources to suggest that in any 
					  software system, larger components are proportionally more 
					  reliable than smaller components.”", 
		"references" : [ 
		"[19] L. Hatton, “Re-examining the Fault Density-Component Size ..." 
		] 
	}, 
	{ 
		"citation" : "Hatton examined a number of data sets, [15], [18] and 
					  concluded that  there was evidence of “macroscopic behavior” 
					  common to  all data sets despite the massive internal 
					  complexity of each  system studied, [19].", 
		"references" : [ 
		"[15] K.H. Moeller and D. Paulish, “An Empirical Investigation of ...", 
		"[18] T. Keller, “Measurements Role in Providing Error-Free Onboard ...", 
		"[19] L. Hatton, “Re-examining the Fault Density-Component Size  ..." 
		] 
	}] 
}
\end{lstlisting}
\end{minipage}

\subsection{Challenges}

One of the challenge during parsing is finding cited sentences that do not contain any specific identifiers. Indeed the task is straightforward when a sentence contains square brackets, for example, `[34]' or `[Ali86]'. However, sometimes a link to bibliography can be composed only from the authors' names. In this case it becomes difficult to distinguish a citation from any other sentence.

Another challenge in using square brackets as citation identifiers arises when square brackets are used not as links to bibliography. For example, a parser might mix up an array in a code snippet with a link to bibliography. Usage of code snippets are common in computer science literature so it would be nice to have a method to distinguishing a snippet of source code from the natural text. 


\section{Indexator}

As an Indexator we use Solr: solution from Apache Software Foundation built on Apache Lucene. Apache Lucene is an open source, information retrieval library that provides indexing and full text search capabilities \footnote{Apache Lucene, http://lucene.apache.org/core/}. While web search engines focus on searching content on the Web, Solr is designed to search content on corporate networks of any form. Some of the public service that use Solr as a server: Instagram (photo and video sharing social network), Netflix (movie hosting service), StubHub.com (public entertainment events ticket reseller). 

Figure \ref{fig:solr} illustrates a high level architecture of Solr. Solr is distributed as a Java web application that runs in any servlet container, for example, Tomcat or Jetty. It provides REST-like web services so external applications can make queries to Solr or index documents. Once the data is uploaded, it goes through text analysis pipeline. In this stage, different inspections to remove duplicates in the data or some document-level operations prior to indexing, or create multiple documents from a single one can be applied. Solr comes with variety of query parser implementations responsible for parsing the queries passed by the end search as a search string. For example, \emph{TermQuery, BooleanQuery, PhraseQuery, PrefixQuery, RangeQuery, MultiTermQuery, FilteredQuery, SpanQuery} and others. Solr has xml configuration files (schema.xml and solrconfig.xml) to define the structure of the index and how fields will be represented and analyzed.

Next section describes configuration of Solr instance for our search system.  

  \begin{figure}[htp]
    \centering
    \includegraphics[scale=0.55 , trim = 0mm 10mm 0mm 10mm]{solr}
    \caption{High level architecture of Solr system}
    \label{fig:solr}
  \end{figure} 

\subsection{Solr Configuration}
Solr installation requires JDK and any servlet container installed on a machine. Here configuration process for Apache Tomcat container is described.
We need to download Solr distribution that can be found on official Solr home page \footnote{Apache Solr, http://lucene.apache.org/solr/}. Solr is distributed as an archive. After unzipping the archive, the extracts have following directories:

\begin{itemize}
	\item \textbf{contrib/} - directory containing additional libraries to Solr, such as Data Import Handler, MapReduce, Apache UIMA, Velocity Template, and so on.
	\item \textbf{dist/} - directory providing distributions of Solr and other useful libraries such as SolrJ, UIMA, MapReduce, and so on.
	\item \textbf{docs/} - directory with documentation for Solr.
	\item \textbf{example/} - Jetty based web application that can be used directly.
	\item \textbf{Licenses/} - directory containing all the licenses of the underlying libraries used by Solr.
\end{itemize}

Copy the dist/solr.war file from the unzipped folder to \$CATALINA\_HOME/webapps/solr.war. Then point out to Solr location of home directory describing a collection. By collection in Apache Solr one imply a collection of Solr documents that represent one complete index. It's possible to choose one of the options:
\begin{itemize}
	\item \textbf{Java options} one can use following command so that the container picks up Solr collection information from the appropriate location:
	\begin{lstlisting}[language=bash]
$export JAVA_OPTS="$JAVA_OPTS -Dsolr.solr.home=/opt/solr/example"
	\end{lstlisting}
	\item \textbf{JNDI lookup} Or it is possible to configure the JNDI lookup for the java:comp/env/solr/home resource by pointing it to the Solr home directory. This can be done by creating a context XML file with some name (context.xml) in \$CATALINA\_HOME/conf/Catalina/localhost/context.xml and adding the following entries:
	\begin{lstlisting}[language=XML]
<?xml version="1.0" encoding="utf-8"?>
<Context docBase="<solr-home>/example/solr/solr.war" debug="0" crossContext="true">
<Environment name="solr/home" type="java.lang.String" value="<solr-home>/example/solr" override="true"/>
</Context>
	\end{lstlisting}
\end{itemize}

The Solr home directory contains configuration files and index-related data. It should consist of three directories:
\begin{itemize}
\item \textbf{conf/} - directory containing configuration files, such as solrconfig.xml and schema.xml
\item \textbf{data/} - default location for storing data related to index generated by Solr
\item \textbf{lib/} - optional directory for additional libraries, used by Solr to resolve any plugins
\end{itemize}

Configuring Solr instance requires defining a Solr schema and configuring Solr parameters.
\paragraph{Defining Solr schema}
Solr schema is defined in schema.xml file placed to conf/ directory of Solr home directory. Solr distribution comes with a sample schema file that can be changed for the needs of the project. Schema file defines the structure of the index, including fields and field types. The basic overall structure of schema file is following:
\begin{lstlisting}[language=XML]
	<schema>
	  <types>
	  <fields>
	  <uniqueKey>
	  <copyField>
	</schema>
\end{lstlisting}

The basic unit of data in Solr is document. Each document in Solr consist of fields that described in schema.xml file. By describing data in schema.xml Solr understand the structure of the data and what actions should be performed to handle this data. Here is an example of a field in schema file:
\begin{lstlisting}[language=XML]
	<field name="id" type="integer" indexed="true" stored="true" required="true"/>
\end{lstlisting}
Table \ref{tab:field.attrs} lists and explains major attributes of field element. 

\begin{table}[ht]
\begin{center}
\begin{tabular}{ | l | p{10cm}  | } 
 \hline
 Name & Description \\  \hline
 Default & default value if it is not read while importing a document \\ \hline
 Indexed & true if field should be indexed \\ \hline
 Stored & when true a field is stored in index store and is accessible while displaying results \\ \hline
 compressed & when true a field will be zipped, applicable for text-type fields \\ \hline
 multiValued & if true, field can contain multiple values in the same document. \\ 
 \hline
\end{tabular}
\caption{Major attributes of field element in schema.xml}
\label{tab:field.attrs}
\end{center}
\end{table}

Here is a fragment of schema file defining fields of a document in Citation Search Engine collection:
\begin{lstlisting}[language=XML]
    <fields>
     <field name="_version_" type="long" indexed="true" stored="true" multiValued="false"/>
     <field name="id" type="string" multiValued="false"/>
     <field name="text" type="text_en" indexed="true" multiValued="false"/>
     <field name="context" type="string" indexed="false" multiValued="false"/>
     <field name="path" type="string" indexed="false" multiValued="false"/>
     <field name="reference" type="string" indexed="false" stored="true" multiValued="true" />
    </fields>
\end{lstlisting}

Every document represent a citation with matching bibliographic references. In schema file we indicate that we want to index text field which is citation text. We store id of a citation, that is a generated value, calculated from the hash of the citation string. Specifying id is particularly useful for updating documents. We also store a context of a citation and path to the scientific article where citation was found. As citation can refer to multiple sources, we make reference field multivalued. 

In schema configuration file one can define field type, for example, string, date or integer and map them to Java class. This can be handy to define custom type. A field type includes following information:

\begin{itemize}
\item Name
\item Implementation class name
\item If field type is TextField, it will include a description of field analysis
\item Field attributes 
\end{itemize}

A sample field type description:
\begin{lstlisting}[language=XML]
	<fieldType name="text_ws" class="solr.TextField" positionIncrementGap="100">
  		<analyzer>
  		<tokenizer class="solr.WhitespaceTokenizerFactory"/>
  		</analyzer>
	</fieldType>
\end{lstlisting}

Other elements in the Solr schema file listed in Table \ref{tab:fields.others}:

\begin{table}[ht]
\begin{center}
\begin{tabular}{ | l | p{10cm}  | } 
 \hline
 Name & Description \\  \hline
 UniqueKey & specifies which field in documents is a unique identifier of a document, should be used if you ever update a document in the index\\ \hline
 copyField & used to copy field's value from one field to another \\ \hline
\end{tabular}
\caption{Description of some elements in schema.xml}
\label{tab:fields.others}
\end{center}
\end{table}
Once the schema is configured next step is to configure instance itself.

\paragraph{Configuring Solr parameters}
To configure Solr instance itself we need to describe solrconfig.xml and solr.xml files.

\begin{description}
\item[solr.xml] The solr.xml configuration is located in solr home directory and used for configuration of logging and advanced options to run Solr in a cloud mode.
\item[solrconfig.xml] The solrconfig.xml configuration file primarily provides you with an access to index-management settings, RequestHandlers, listeners, and request dispatchers. The file has a number of complex sections and mainly is changed when a specific need encounter. 
\end{description} 


 %\cite{solr.config}

\subsection{Enhanced search features}
Solr provides a number of additional features that can enhance search system. One of the feature we use is synonyms. To use this feature you need to specify synonyms.txt file with listed synonyms. This file is used by synonym filter to replace words with their synonyms. For example, a search for "DVD"may expand to "DVD", "DVDs", "Digital Versatile Disk" depending on the mapping in this file. This file can be also used for spelling corrections. Here is an example of synonyms.txt file:
\begin{lstlisting}
GB, gib, gigabyte, gigabytes
MB, mib, megabyte, megabytes
Television, Televisions, TV, TVs
Incident_error, error
\end{lstlisting}

Additionally, there are other configuration files that appear in the configuration directory. We are listing them in Table \ref{tab:solr.config} with the description of each configuration:
\begin{table}[ht]
\begin{center}
\begin{tabular}{ | l | p{10cm}  | } 
 \hline
 Name & Description \\  \hline
 protwords.txt & file where you can specify protected words that you do not wish to get stemmed. So, for example, a stemmer might stem the word "catfish" to "cat" or "fish".\\ \hline
 spellings.txt & file where you can provide spelling suggestions to the end user. \\ \hline
 elevate.txt & file  where you can change the search results by making your own results among the top-ranked results. This overrides standard ranking scheme, taking into account elevations from this file. \\ \hline
 stopwords.txt & Stopwords are those that will not be indexed and used by Solr in the applications. This is particularly helpful when you really wish to get rid of certain words. For example, in the string, "Jamie and joseph," the word "and" can be marked as a stopword. \\ \hline
\end{tabular}
\caption{Additional configuration files in Solr}
\label{tab:solr.config}
\end{center}
\end{table}

\section{Meta Information storage}
As a meta information storage we used MongoDB. MongoDB is a NoSQL document oriented storage written in C++. Data in MongoDB is stored in JSON-like documents with a dynamic schema. The format of stored data is called BSON, which stands for Binary JSON. BSON is an open standard developed for human readable data exchange \footnote{BSON specification, http://bsonspec.org/}. MongoDB requires a little amount of configuration to start to work with. 

\subsection{Configuration}
Once MonogDB distributive is downloaded is very easy to set up a database server. All we need to start the MongoDB server is to type \emph{mongod} command. In our case we would like to specify database location with --dbpath parameter and default listening port:
\begin{lstlisting}[language=Bash]
	> mongod --dbpath /home/aliya/mongodb2 --port 27272
\end{lstlisting}

MongoDB provides REST API, to enable REST API use parameter --rest:
\begin{lstlisting}[language=Bash]
	> mongod --dbpath /home/aliya/mongodb2 --port 27272 --rest true
\end{lstlisting}

The simple way to communicate with MongoDB server is to use mongo shell, in our case we specify --port parameter to connect to our instance of MongoDB:
\begin{lstlisting}[language=Bash]
	> mongo --port 27272
\end{lstlisting}

Compared to relational databases MongoDB operates with terms collection, which is equivalent to table, and document, which is equivalent to record in relational databases. MongoDB doesn't require creating databases and collections explicitly. Databases and collections can be created while starting to use MongoDB. To see list of databases or collections, type show dbs in mongo shell:
 \begin{lstlisting}[language=Bash]
	> show dbs
\end{lstlisting}
Mongo shell allows to make queries, updates, deletes on collections, get various statistics on data and server usage, and manipulate with data with map-reduce interface, full documentation can be found on the official web site \footnote{MongoDB database, http://www.mongodb.org/}. 

\section{Web search interface}
Web Search interface is a Java web application running in a servlet container. Figure \ref{fig:mvc} shows an architecture of a web search application. The application is based on MVC (model-view-controller) architectural pattern implemented with Struts framework \footnote{Struts framework, https://struts.apache.org/}. The application communicates with Solr via Solr REST API and with Mongo database via Java database connector.  

  \begin{figure}[htp]
    \centering
    \includegraphics[scale=0.6, trim = 0mm 20mm 0mm 10mm]{mvc}
    \caption{Architecture overview of web search application}
    \label{fig:mvc}
  \end{figure}  

\subsection{Citation search page}
\subsection{Details page}

\chapter {Validation}
\label{cha:validation}
In which you show how well the solution works.

\chapter {Conclusion and Future Work}
\label{cha:conclusion}
In which we step back, have a critical look at the entire work, then conclude, and learn what lays beyond this thesis.

%%CS can be used as a search engine for the repository of scientific papers in research institutes, in universities, or in any
%%organization that deals with collection of scientific papers. /Users/aliya/Documents/Master Thesis/template/scgbib/LatexTemplates/msc-thesis/preamble.tex
%Parser enchancement
%meta information by advanced pdf-parsers
%personal storage
%distributed solution

%END Doc
%-------------------------------------------------------

\bibliographystyle{plain}
\bibliography{thesis}

\appendix
\chapter{User guide for Citation Search Engine deployment}
\section{Solr Installation}
\section{MongoDB Installation}
\section{Running parser}
\section{Search interface deployment}
\end{document}