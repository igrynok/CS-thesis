 \documentclass[11pt, oneside,a4paper]{book}
\pagestyle{headings}

\input{preamble}

% A B S T R A C T
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\chapter*{\centering Abstract}
\begin{quotation}
\noindent 
Nowadays the amount of documents in World Wide Web grows exponentially. Tools that can facilitate information retrieval present a particular interest in the modern world. A typical web search engine that search
for information in World Wide Web is a software system that performs full-text indexing without considering meta information. This paper is devoted to the design of the academic paper search engine that takes advantage of meta-information, specifically citations. It is believed that citation is a very concise statement describing the source it refers to. Retrieving such statements can be particularly useful in writing scientific papers, for example, to build up a good argument.
This paper describes implementation of Citation Search Engine, a system that makes an attempt to automatically extract, index and aggregate citations from a set of scientific articles in PDF format. Besides it analyses the results of the deployment of the system on the collection of scientific papers provided by Software Composition Group.

\end{quotation}
\clearpage


% C O N T E N T S 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% NEW CHAPTER %%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{cha:introduction}

\section{Thesis statement}
We believe that considering meta information helps to build enhanced search systems that can facilitate information retrieval. Particularly, we target information retrieval for scientific papers. We consider citations as important text blocks summarising or judging previous scientific findings assisting in creating a new scientific work. We propose Citation Search Engine a software system that extracts citations from scientific papers, aggregates citations based on the referred source, then indexes extracted content. It provides a practical web interface that allows users to search for citations. 

\section{Goals}
We set following goals:
\begin{itemize}
	\item Introduce the state of the art techniques in information search.
	\item Explore the structure of scientific articles, reveal common patterns
	\item Design and implement the academic search engine.
	\item Deploy the system on the given collection of scientific papers
	\item Analyse results, define future work
\end{itemize}

\section{Outline}
The rest of the paper structured as follows:
\begin{description}
	\item[Chapter 2] The chapter first gives a high overview of the architecture of a typical web search engine and shortly reviews popular academic search engines. Then It describes the main steps for inverted index construction.
	%\item[Chapter 3] Devoted to the exploring the structure of scientific papers and identifying parsing challenges.
	\item[Chapter 3] Describes the design of Citation Search Engine. It first shows overall architecture of the proposed system and then describes details of implementation of each component. 
	\item[Chapter 4] The chapter describes the deployment process and analysis the result of setting up the system on the given collection of scientific articles.
	\item[Chapter 5] Evaluation
	\item[Chapter 6] Contains conclusion and possible future work. 
\end{description}

\chapter {Related Work}
\section{Typical web search engine} 

Figure \ref{fig:web-search} illustrates a high level architecture of a standard web engine. It consist of three main components:
\begin{itemize}
	\item Web Crawler
	\item Data indexer
	\item Search interface
\end{itemize} 

  \begin{figure}[htp]
    \centering
    \includegraphics{web-search-engine}
    \caption{A high-level architecture of a typical web search engine}
    \label{fig:web-search}
  \end{figure}

Web Crawler is a program that browses the World Wide Web reading the content of web pages in order to provide up-to-date data to Data Indexer. Data Indexer decides how a page content should be stored in an index database. Index helps to quickly query information. Users can search and view query results through Search Interface. When user makes a query the search engine analysis its index and returns best matched web pages according to specific to indexer criteria. 

Web crawlers that fetch web pages with the content in the same domain are called focused or topical crawlers. An example of focused crawlers are academic-focused crawlers that crawls academical documents. Such crawlers become components of the "focused" search engines. Next chapter reviews some of popular academical search engines.  

\section{Popular academic search engines}
\paragraph{CiteSeer\textsuperscript{x}}
CiteSeer\textsuperscript{x} is an autonomous citation search engine \cite{citeseer, Giles:1998:CAC:276675.276685}. CiteSeer\textsuperscript{x} automatically parses and index publicly available scientific articles found on the World Wide Web. It uses the impact of citations to rank documents.
CiteSeer\textsuperscript{x} is built on the open source infrastructure SeerSuite \cite{seersuite} and uses Apache Solr \cite{solr} search platform for indexing documents. It can extract meta information from papers such as title, authors, abstract, citations. The extraction methods are based on machine learning approaches such ParseCit \cite{Councill08parscit:an}. 
CiteSeer\textsuperscript{x}  one of the world's top repositories and was rated number 1 in July 2010 \cite{web-repos}. It currently has over 4 million documents with nearly 4 million unique authors and 80 million citations. 
CiteSeer\textsuperscript{x} focuses on indexing citations more precisely bibliographic links while in Citation Search Engine we intend to index text of the citations in a body of a document.
\\ \\
\textbf{Google Scholar} 
Google Scholar is a freely accessible web search engine that makes full-text indexing of scientific literature \cite{google.scholar}. Among features of Google Scholar engine are unique ranking algorithm, "cited by" feature, allowing to view abstracts of the articles citied the given article, "related articles" feature, showing the list of closely related articles.
Google Scholar contains roughly 160 million of documents by May 2014 \cite{orduna2014size}. 

\section{Inverted index}
Search engines like CiteSeer or Google Scholar deal with a large collection of documents. When user makes a query to such systems one would like to have a mechanism to process document collections quickly. The way to avoid linear scanning the text of all documents for each query is to \emph{index} them in advance. Thereby we are coming to the concept of \emph{inverted index} that is the major concept in information retrieval. The term \emph{inverted index} comes from the data structure storing a mapping from content, such as words or numbers, to the parts of a document where it occurs. Figure \ref{fig:inverted-index} shows the basic idea of an inverted index. We have a dictionary of terms appearing in the documents. Each term maps to a list that records which documents the term occurs in. Each item in the list, conventionally named as \emph{posting}, records that a term appears in a document, often it records the position of the term in the document as well. The dictionary on Figure \ref{fig:inverted-index} has been sorted alphabetically and each postings list is sorted by document ID. Document ID is a unique number that can be assigned to a document when it's first encountered. \\
  \begin{figure}[htp]
    \centering
    \includegraphics[scale=0.55]{inverted-index}
    \caption{Inverted index example}
    \label{fig:inverted-index}
  \end{figure}
  
 The construction of the inverted index has following steps:
 
\begin{enumerate}
	\item Documents collection
	\item Breaking each document into tokens, turning a document into a list of tokens
  	\item Linguistic preprocessing of a list of tokens into normalised list of tokens
	\item Index documents by creating  an inverted index, consisting of a dictionary and postings
\end{enumerate}

First step of the index construction is documents collection that aims to obtain a set of documents containing sequence of characters. Usually the input to the indexing process is digital documents that are bytes in a file or a web server. While for the plain English text in ASCII encoding solution is straightforward there might be trickier cases. Consider for example a collection of PDF files, we need to correctly decode out characters of some binary representation. Finally, the textual part of the document may need to be extracted out of other parts that will not be processed. Next we should determine a document unit, for example it might be a chapter in a book, or a paragraph in a scientific article. 

After getting the sequence of characters in document units next step is to breaking up documents into \emph{tokens}. Token can be think of a semantical unit for processing, for example, it might be a word or a number. At the same time during tokenisation some characters like punctuations can be thrown out. 

Here is a tokenisation example:

\hspace{50pt}\textcolor[rgb]{0,0,1}{ Input: Friends, Romans, Countrymen, lend me your ears; }

\hspace{50pt}\textcolor[rgb]{0,0,1}{Output: \framebox{Friends} \framebox{Romans} \framebox{Countrymen} \framebox{lend} \framebox{me} \framebox{your} \framebox{ears}} \\

The third step is normalisation. It's good when tokens in a user query match tokens in the token list of documents. Consider an example of querying the word \emph{co-operation}, a user might also be interested in getting documents containing \emph{cooperaion}. \emph{Token normalisation} is a process of turning a token into a canonical form so matches can occur despite superficial differences in the character sequences. One way of token normalisation is keeping relations between unnormalised tokens, which can be extended to manual constructed synonym lists, such as \emph{car} and \emph{automobile}. The most standard way of token normalisation however is creating \emph{equivalence classes}. If tokens become identical after applying a set of rules then they are the equivalence classes. Consider some common normalisation rules that are often used:

\begin{description}
	\item[Stemming and lemmatisation] Words can be used in different grammatical forms, for instance, \emph{organize}, \emph{organizes}, \emph{organizing}, however in many cases it sounds reasonable for one of these words to return documents contain other forms of the word. The goal of stemming and lemmatisation is reduce a form of the word to a common base form. 
	
Here is an example: 
	
\hspace{100pt} \textcolor[rgb]{0,0,1}{ am, are, is = \textgreater be }

\hspace{100pt} \textcolor[rgb]{0,0,1}{car, cars, car's, cars'  =\textgreater car}

The result of applying the rule to the sentense:

\hspace{50pt} \textcolor[rgb]{0,0,1}{the boyÕs cars are different colors   =\textgreater the boy car be differ color}

Stemming and lemmatisation are closely related concepts however there is a difference. \emph{Lemmatisation} usually refers to finding a \emph{lemma}, common base of a word, with the help of a vocabulary, morphological analysis of a word and requires understanding the context of a word and language grammar. \emph{Stemming} however operates with a word without knowing it context and thereby can't distinguish the same words having different meanings depending on the context. 

Here is an example:

\hspace{10pt} \textcolor[rgb]{0,0,1}{ better = \textgreater good }, can only be matched by lemmatisation since requires dictionary look-up

\hspace{10pt} \textcolor[rgb]{0,0,1}{writing  =\textgreater write}, can be matched by both lemmatisation and stemming

\hspace{10pt} \textcolor[rgb]{0,0,1}{meeting  =\textgreater meeting(noun) or to meet(verb)}, can be matched only by lemmatisation since requires the word context 

In general stemmers are easier to implement and run faster. The most common algorithm for stemming is \emph{Porter's} algorithm \cite{Porter:1997:ASS:275537.275705}.
	  
	\item[Capitalization/case-folding] A simple strategy is to reduce all letters to a lower case, so that sentences with \emph{Automobile} will match to queries with \emph{automobile}., however this approach would not be appropriate for example to identifying company names, such as \emph{General Motors}. The better strategy for English language would be to lowercase words only in the beginning of the sentences and to lowercase all words in titles. Case-folding can be be done more accurately by a machine learning model using more features to identify weather a words should be lowercased.
	\item[Accents and diacritics]  Diacritics in English language play an insignificant role and simply can be removed. For instance \emph{clich\'e} can be substituted by \emph{cliche}. In other languages diacritics can be part of the writing system and distinguish different sounds.  However in many cases users can enter queries for words without diacritics, whether for reasons of speed, laziness or limited software 
\end{description}

The last step is the core part of the building inverted index. The input to indexing is a list of normalised tokens for each document, which is a list of pairs of
term and document ID, as on Figure \ref{fig:input-indexing}. The indexing algorithm is sorting the input list so that the terms are in alphabetical order. Then it merges the same terms from the same document. And finally instances of the same term are grouped and the result is split into a dictionary and postings, as shown on Figure \ref{fig:inverted-index}.

  \begin{figure}[htp]
    \centering
    \includegraphics[scale=0.5]{input-indexing}
    \caption{Input to the indexing algorithm}
    \label{fig:input-indexing}
  \end{figure}

%%Since a term generally occurs in a number of documents, this data organization already reduces the storage requirements of the index. 

\section{Dynamic indexing}
So far we assumed that document collection is static however there are many cases when collection can be updated, for example, by adding new documents, deleting or updating existing documents. One simple way dealing with dynamic collection is to reconstruct the inverted index from scratch. This might be acceptable if changes made in collection are small over time and delay in making new documents searchable is not critical. However if there is one of a requirement mentioned above, for example, making new documents searchable quickly then one might be interested in another solution: keeping auxiliary index. Thus we have a large main index and we keep auxiliary index for changes. The auxiliary index is kept in memory. Every time a user makes a query the search runs over both of the indexes and results are merged. When auxiliary index becomes too large it can be merged with the main index.

\section{Retrieving search results}
When a user makes a query she would be interested in getting a result document containing all terms in the query so that terms are close to each other. Consider an example of querying a phrase containing 4 terms. The part of the document that contains all terms is named a \emph{window}. The size of the window is measured in a number of words. For instance a smallest  window for 4-terms query will be 4. Intuitively, smaller windows represent better results for users. Such window can become one of the indicators scoring a document in the search result. If there is no document containing all 4 terms, a 3-term phrase can be queried. Usually search systems hides the complexity of searching a result from user introducing \emph{free text query parsers} so a user can make only one query.     

\section{Conclusion} 
Figure \ref{fig:search-full-diagram} summarises approaches described above in a more detailed picture of a basic search system. 
Left stream in the Figure \ref{fig:search-full-diagram} describes the process of parsing a set of documents and applying linguistic processings (tokenisation and lemmatisation) in order to built indexes with the help of a indexer. The middle stream on Figure \ref{fig:search-full-diagram} represents a user making a query, where free text parsers together with spell checkers send requests to the index bank. The index bank returns document candidates to a scoring and ranking component, the left stream on Figure \ref{fig:search-full-diagram}. Finally, ranked document are shown to the user as a result page.    

  \begin{figure}[htp]
    \centering
    \includegraphics[scale=0.6]{search-full-diagram}
    \caption{Search Engine internals}
    \label{fig:search-full-diagram}
  \end{figure}

%\chapter{Problem}
%In which we understand what the problem is in detail.

\chapter {Citation Search Engine}

\section{System overview}

The overall architecture of Citation Search Engine is shown on Figure \ref{fig:cs}. There are three main operations performed by Citation Search Engine: parsing PDF files, indexing documents and querying on resulted indexes. Correspondingly there are three major components responsible for accomplishment of these operations: \emph{Parser}, \emph{Indexer Solution} and \emph{Search Web App}. The system has two more components for storing data: \emph{Indexes Storage} for storing indexes and \emph{Meta Information Storage} for storing meta information. For the convenience, the workflow of the system is numbered and will be explained below.

  \begin{figure}[htp]
    \centering
    \includegraphics[scale=0.6]{cs}
    \caption{Search Engine high level architecture}
    \label{fig:cs}
  \end{figure}

First operation performed by the system is indexing. In the indexing process collection of PDF files is provided to \emph{Parser} (1). \emph{Parser} parses PDF files into text and extract meta information from text. It then packs data about citations into documents and publishes obtained documents to \emph{Indexer Solution} (2) which indexes and stores indexes in \emph{Indexes Storage} (2'). Meta information extracted from textual representation of PDF files that doesn't require indexing is stored in \emph{Meta Information Storage} (3). 

In the querying process user searches for some phrase using \emph{Search Web App} user interface. The phrase is analysed by \emph{Indexer Solution}. \emph{Indexer Solution} finds documents matching the query phrase in \emph{Indexes Storage} (2') and sends them to \emph{Search Web App} (4). If user is interested in getting details regarding the specific reference the information will be looked up in \emph{Meta Information Storage} (5). Finally, the result is shown to the user as a an html page (6). 

Next sections of the chapter describe implementation of each component in detail and show the reasons of choosing a particular solution.       

\section{Parser}

It is practical to divide the work of \emph{Parser} into two phases: \emph{Processing} and \emph{Publishing}, as on Figure \ref{fig:parser}. The output of \emph{Processing} phase is input to \emph{Publishing} phase. \emph{Processing} phase aims to parse PDF-files and prepare documents with extracted information for publishing. Next parts of this section describes \emph{Processing} and \emph{Publishing} phases in detail. 

  \begin{figure}[htp]
    \centering
    \includegraphics[scale=0.6]{parser}
    \caption{Parser workflow}
    \label{fig:parser}
  \end{figure}

\subsection{Processing}
The main role of \emph{Processing} phase is to parse scientific articles into text and extract citations and bibliographic references to create documents for publishing. 
In common case parsing PDF-files from different sources is a very challenging task as there is a large number of scientific articles in different formats. Besides that there exist scientific articles written decades ago which can use different encoding algorithms than articles created recently. Thereby building a universal parser is practically impossible. In our case we try to identify common patterns covering majority of scientific article formats or at least formats found in our collection of articles.  

\emph{Processing} phase starts with recursively walking though directory tree of the collection library. While walking though directory \emph{Parser} filters non-pdf files and parses and processes each PDF-file separately. We used Apache PDFBox library \cite{pdfbox} to parse PDF-files into text. The library extracts full text from PDF-file but without any hints on the initial structure of the article. In our case to find citations and bibliographic references in text we need to look them up in different parts of the article, so it was designed an algorithm to break PDF-text into sections.

Generally we are interested in identifying a body of the document where we can find citations and references block where we can find bibliographic links. One way of finding these sections can be using keywords that might signify the beginning or the end of some section. Based on keywords on can extract different sections of a document. Figure \ref{fig:parser} shows a sample text of a parsed pdf-document with keywords. 

  \begin{figure}[htp]
    \centering
    \includegraphics[scale=0.6]{template}
    \caption{Sample text of the parsed scientific article. Keywords help to break the document into sections.}
    \label{fig:template}
  \end{figure}
  
One can notice following characteristics of scientific articles:
\begin{itemize}
	\item Body of a document comes before references section
	\item Appendix or Author's biography sections can come after references section 
	\item Each document contains an Abstract and References words and might contain Appendix word. We call these words keywords. 
\end{itemize}  

In common case keywords can be written in different formats, for example, using upper or lower case. Table \ref{tab:keywords} illustrates variations of keywords in the beginning of document sections.

\begin{table}[ht]
\begin{center}
\begin{tabular}{ | l | l | l | } 
 \hline
 body & references & appendix \\  \hline
 Abstract & References & Appendix \\ 
 ABSTRACT & References: & APPENDIX \\
 & REFERENCES & \\  
 \hline
\end{tabular}
\caption{Keywords identifying different sections in a document}
\label{tab:keywords}
\end{center}
\end{table}

After breaking down a document into sections as show on Figure \ref{fig:template}, text in sections are presented in one-column format. There are two aspects regarding this format. First, sentences can be splitted by new line symbol in the end of a line. Secondly, words can be splitted by dash symbol in the end of a line. It was introduce a normalization step where new lines are substituted by white spaces and dashes are removed in the end of a line to obtain continues text.

As a result of normalization step we have a document divided into body and references section. Before searching citations in the body of a document it is reasonable to break body into sentences. In general, breaking text into sentences is not an easy task. Consider a simple example with a period. Period not only indicates the end of a sentence but also can encounter in a sentence itself, for example, an item of a numbered list or a name of a scientist. Besides, not all sentences end by period, for example, title of a section or an item of a list. It was used Stanford CoreNLP library that employs natural language and machine learning techniques to extract sentences \cite{stanford.nlp}. 

Next, we search for citations in a body of a document and bibliography links in a references section. When an author makes a citation it puts a link to a bibliographic reference in the sentence. It is common to use square brackets ([ and ]) to make a link to a bibliographic reference in the sentence. Thus, detecting square brackets in the text we can identify citations. After analyzing some set of articles it was revealed frequent patterns in using square brackets for citing showed in Table \ref{tab:citations}.

\begin{table}[ht]
\begin{center}
\begin{tabular}{ | l | p{7cm} | } 
 \hline
 Patterns of using [ ] & Example in text \\  \hline
 {[}21{]} & Our conclusion is that, contrary to prior pessimism \textbf{[21]}, [22], data mining static code attributes to learn defect predictors is useful. \\ 
 {[}20, 3, 11, 17{]} & In the nineties, researchers focused on specialized multivariate models, i.e., models based on sets of metrics selected for specific application areas and particular development environments \textbf{[20, 3, 11, 17]}. \\
 {[}24, Sections 6.3 and 6.4 {]}& Details on the life-cycle of a bug can be found in the BUGZILLA documentation \textbf{[24, Sections 6.3 and 6.4]}. \\
 {[}PJe02{]} & In a lazy language like Haskell \textbf{[PJe02]} this is not an issue - which is one key reason Haskell is very good at defining domain specific languages.  \\
 \hline
\end{tabular}
\caption{Frequent patterns in using square brackets ([ and ])for citing}
\label{tab:citations}
\end{center}
\end{table}

Further we need to extract bibliographic links from a references section. For that we studied most common variants of composing references sections. Table \ref{tab:references} surmises these findings.

\begin{table}[h]
\begin{center}
\begin{tabular}{ | p{12cm} | } 
 \hline
 References section templates\\  \hline
 \textbf{[1]} J. Bach. Useful features of a test automation system (partiii) \ldots \newline
 \textbf{[2]} B. Beizer. Black-Box Testing. John Wiley and Sons, \ldots \newline
 \ldots \\ \hline 
 \textbf{1.}  J. R. Hobbs,  Granularity, Ninth International Joint Conference \ldots \newline
  \textbf{2.}  W. Woods, What's in a Link:  Foundations for Semantic Networks, \ldots \newline
  \ldots \\ \hline
  \textbf{[1].}   Arnold, R.S., Software Reengineering, ed. \ldots \newline
  \textbf{[2].}   Larman, C., Applying  UML and Patterns. 1998, \ldots \newline  
  \ldots \\ \hline
  \textbf{[ASU86]} A. V. Aho, R. Sethi, and J. D. Ullman. Compilers: \ldots \newline
  \textbf{[AU73]} A.V. Aho and J.D. Ullman. The theory of parsing, translation \ldots \newline 
  \ldots \\
 \hline
\end{tabular}
\caption{Frequent patterns of writing references block}
\label{tab:references}
\end{center}
\end{table}

Extracting bibliographic links allow us to match citations with bibliographic links and prepare documents for publishing. 

\subsection{Publishing}

There are two systems where documents will be published: Solr and MongoDB. As already explained above Solr is used for indexing citations and MongoDB for storing meta-data. Some reasonings in favor of using MongoDB additionally to Solr storage will be given below.

\subsection{Publishing to Solr}
The common way to interact with Solr is using REST API. Solr provides client libraries for many programming languages to handle interactions with Solr's REST API. In our project we used SolrJ client library for Java language. This library abstract interaction with Solr into java objects instead of using typical XML request/response format. Basic Solr storage unit is called document and SolrJ has document abstraction implementation. For every detected citation we compose a document to publish. Figure \ref{fig:doc} represents a structure of documents we publish to Solr.

  \begin{figure}[htp]
    \centering
    \includegraphics[scale=0.7]{doc}
    \caption{Document structure publishing to Solr}
    \label{fig:doc}
  \end{figure} 

Every document representing one citation consist of following fields:
\begin{itemize}[noitemsep]
  \item id: document unique id, mandatory field for publishing to Solr
  \item text: text of the citation that we want to index
  \item context: citation with a text framing it, we take 1 sentence before and 1 after the citation 
  \item path: URL of a document where citation was found
  \item references: list of bibliographic links from references section matching this citation
\end{itemize}

Lets have a look on how Solr stores documents. Solr uses NoSql like data storage, but actually it is even more limited than traditional NoSql databases. Indeed the data stored in Solr is very flat, which means that Solr doesn't allow to store hierarchical data as well as relational date. In our case along with the reference we are interested in storing a title of the scientific article parsed from the reference, so later we can aggregate citations referred to the same scientific article.  

\subsection{Publishing to MongoDB}   

\subsection{Challenges}
\section{Indexator}
\subsection{Configuration}
\subsection{Enhanced search features}
\section{Meta Information storage}
\section{Web search interface}
\subsection{Citation search page}
\subsection{Details page}

\chapter {Validation}
In which you show how well the solution works.

\chapter {Conclusion and Future Work}
In which we step back, have a critical look at the entire work, then conclude, and learn what lays beyond this thesis.

%%CS can be used as a search engine for the repository of scientific papers in research institutes, in universities, or in any
%%organization that deals with collection of scientific papers. /Users/aliya/Documents/Master Thesis/template/scgbib/LatexTemplates/msc-thesis/preamble.tex



%END Doc
%-------------------------------------------------------

\bibliographystyle{plain}
\bibliography{thesis}


\end{document}